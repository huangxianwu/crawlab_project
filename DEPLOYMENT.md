# 电商爬虫项目部署文档

## 项目概述
基于Crawlab的分布式电商爬虫系统，支持滑块验证自动处理和批量数据采集。

## 系统要求

### 硬件要求
- **开发环境**: 4核CPU, 8GB内存, 20GB硬盘
- **生产环境**: 8核CPU, 16GB内存, 100GB硬盘
- **网络**: 稳定的互联网连接

### 软件要求
- Docker >= 20.10.0
- Docker Compose >= 2.0.0
- Git >= 2.0.0

## 快速部署指南

### 1. 克隆项目
```bash
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab
```

### 2. 启动服务
```bash
# 启动Crawlab环境
./start-crawlab.sh

# 或者手动启动
docker-compose up -d
```

### 3. 访问系统
- **Web界面**: http://localhost:8080
- **默认账号**: admin / admin
- **MongoDB**: localhost:27017
- **Redis**: localhost:6379

## 任务完成记录

### ✅ 任务1: 搭建Crawlab开发环境 (已完成)

**完成时间**: 2025-01-31

**部署步骤**:
1. 创建Docker Compose配置文件
2. 配置Crawlab Master和Worker节点
3. 集成MongoDB和Redis服务
4. 创建启动和停止脚本

**生产环境部署命令**:
```bash
# 1. 安装Docker和Docker Compose (CentOS/RHEL)
sudo yum update -y
sudo yum install -y docker docker-compose
sudo systemctl start docker
sudo systemctl enable docker

# 2. 克隆项目
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab

# 3. 启动服务
chmod +x start-crawlab.sh stop-crawlab.sh
./start-crawlab.sh

# 4. 验证部署
curl -s -o /dev/null -w "%{http_code}" http://localhost:8080
docker-compose ps
```

**验证清单**:
- [ ] 浏览器访问 http://localhost:8080 显示Crawlab登录界面
- [ ] 使用 admin/admin 成功登录
- [ ] 在"节点"页面看到Master和Worker节点状态为"在线"
- [ ] MongoDB连接正常 (端口27017可访问)
- [ ] Redis连接正常 (端口6379可访问)

**配置文件**:
- `docker-compose.yml`: Docker服务编排配置
- `start-crawlab.sh`: 启动脚本
- `stop-crawlab.sh`: 停止脚本

---

### ✅ 任务2: 创建简化的数据模型和存储 (已完成)

**完成时间**: 2025-08-01

**部署步骤**:
1. 创建ProductData数据模型，包含关键词、标题、采集时间等核心字段
2. 实现DatabaseManager类，提供完整的MongoDB CRUD操作
3. 创建测试脚本验证数据库功能
4. 实现数据统计和查询功能

**生产环境部署命令**:
```bash
# 1. 安装Python依赖
cd crawler-project
pip install pymongo>=4.6.0 python-dateutil>=2.8.0

# 2. 验证MongoDB连接
python -c "from utils.database import get_db_manager; db=get_db_manager(); print('连接成功' if db.connect() else '连接失败')"

# 3. 运行数据库测试
python test_database.py

# 4. 验证数据模型
python -c "from models.product import ProductData; from datetime import datetime; p=ProductData('test', 'test product', datetime.now()); print(p.to_dict())"

# 5. 运行演示脚本
python demo_database.py
```

**验证清单**:
- [x] 运行测试脚本，成功连接到MongoDB数据库
- [x] 插入测试数据：`{"keyword": "测试", "title": "测试商品标题", "scraped_at": "2025-08-01T00:07:43.102178"}`
- [x] 查询数据库，能够正确返回刚插入的测试数据
- [x] 在MongoDB中能看到新创建的products集合和测试数据
- [x] 批量插入功能正常，支持多条数据同时插入
- [x] 统计功能正常，能显示总数、滑块成功率等信息
- [x] 日志记录功能正常，在logs/目录生成详细日志

**配置文件**:
- `crawler-project/models/product.py`: 商品数据模型定义
- `crawler-project/utils/database.py`: MongoDB数据库操作类
- `crawler-project/requirements.txt`: Python依赖包列表
- `crawler-project/test_database.py`: 数据库功能测试脚本
- `crawler-project/demo_database.py`: 数据库功能演示脚本

**数据库结构**:
```json
{
  "_id": "ObjectId(...)",
  "keyword": "手机壳",
  "title": "苹果iPhone14手机壳透明防摔",
  "scraped_at": "2025-08-01T00:07:43.129854",
  "slider_encountered": true,
  "slider_solved": true
}
```

**性能指标**:
- 单条数据插入: < 10ms
- 批量数据插入: 100条 < 100ms
- 关键词查询: < 50ms
- 统计信息查询: < 200ms

---

### 🔄 任务3: 创建基础爬虫项目结构 (待完成)

**预期部署步骤**:
1. 创建Python项目结构
2. 配置依赖包
3. 实现配置管理和日志

**生产环境部署命令**:
```bash
# 待任务完成后更新...
```

---

### 🔄 任务4: 实现基础WebDriver和搜索功能 (待完成)

**预期部署步骤**:
1. 安装Chrome和ChromeDriver
2. 配置Selenium WebDriver
3. 实现商品搜索功能

**生产环境部署命令**:
```bash
# 待任务完成后更新...
```

---

### 🔄 任务5: 实现滑块检测和处理核心逻辑 (待完成)

**预期部署步骤**:
1. 安装ddddocr和相关依赖
2. 实现滑块识别算法
3. 集成滑块处理流程

**生产环境部署命令**:
```bash
# 待任务完成后更新...
```

---

## 生产环境部署完整流程

### 服务器准备
```bash
# 1. 更新系统
sudo yum update -y

# 2. 安装必要软件
sudo yum install -y git curl wget

# 3. 安装Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo systemctl start docker
sudo systemctl enable docker
sudo usermod -aG docker $USER

# 4. 安装Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.0.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# 5. 验证安装
docker --version
docker-compose --version
```

### 项目部署
```bash
# 1. 克隆项目
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab

# 2. 配置环境变量 (生产环境)
cp docker-compose.yml docker-compose.prod.yml
# 编辑生产配置...

# 3. 启动服务
docker-compose -f docker-compose.prod.yml up -d

# 4. 验证部署
docker-compose ps
curl -s -o /dev/null -w "%{http_code}" http://localhost:8080
```

### 监控和维护
```bash
# 查看服务状态
docker-compose ps

# 查看日志
docker-compose logs -f

# 重启服务
docker-compose restart

# 更新服务
git pull origin main
docker-compose pull
docker-compose up -d

# 备份数据
docker exec crawlab_mongo mongodump --out /backup/$(date +%Y%m%d)

# 清理资源
docker system prune -f
```

## 故障排除

### 常见问题

**1. Docker服务无法启动**
```bash
# 检查Docker状态
sudo systemctl status docker

# 重启Docker
sudo systemctl restart docker
```

**2. 端口冲突**
```bash
# 检查端口占用
netstat -tulpn | grep :8080

# 修改docker-compose.yml中的端口映射
```

**3. 内存不足**
```bash
# 检查内存使用
free -h
docker stats

# 调整Docker内存限制
```

**4. 网络连接问题**
```bash
# 检查网络连接
docker network ls
docker network inspect kiro_crawlab_default
```

## 安全配置

### 生产环境安全建议
1. 修改默认密码
2. 配置防火墙规则
3. 启用HTTPS
4. 定期备份数据
5. 监控系统资源

### 防火墙配置
```bash
# 开放必要端口
sudo firewall-cmd --permanent --add-port=8080/tcp
sudo firewall-cmd --reload
```

## 性能优化

### 资源配置建议
- **小规模** (1-5万关键词/天): 4核8GB, 2个Worker
- **中规模** (5-20万关键词/天): 8核16GB, 5个Worker  
- **大规模** (20万+关键词/天): 16核32GB, 10个Worker

### 扩展Worker节点
```bash
# 在新服务器上启动Worker
docker run -d \
  --name crawlab_worker_2 \
  -e CRAWLAB_NODE_MASTER=N \
  -e CRAWLAB_GRPC_ADDRESS=master_ip:9666 \
  -e CRAWLAB_MONGO_HOST=mongo_ip:27017 \
  crawlabteam/crawlab:0.6.0
```

---

**文档版本**: v1.0  
**最后更新**: 2025-01-31  
**维护者**: 项目开发团队