# ç”µå•†çˆ¬è™«é¡¹ç›®éƒ¨ç½²æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°
åŸºäºCrawlabçš„åˆ†å¸ƒå¼ç”µå•†çˆ¬è™«ç³»ç»Ÿï¼Œæ”¯æŒæ»‘å—éªŒè¯è‡ªåŠ¨å¤„ç†å’Œæ‰¹é‡æ•°æ®é‡‡é›†ã€‚

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **å¼€å‘ç¯å¢ƒ**: 4æ ¸CPU, 8GBå†…å­˜, 20GBç¡¬ç›˜
- **ç”Ÿäº§ç¯å¢ƒ**: 8æ ¸CPU, 16GBå†…å­˜, 100GBç¡¬ç›˜
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

### è½¯ä»¶è¦æ±‚
- Docker >= 20.10.0
- Docker Compose >= 2.0.0
- Git >= 2.0.0

## å¿«é€Ÿéƒ¨ç½²æŒ‡å—

### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab
```

### 2. å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨Crawlabç¯å¢ƒ
./start-crawlab.sh

# æˆ–è€…æ‰‹åŠ¨å¯åŠ¨
docker-compose up -d
```

### 3. è®¿é—®ç³»ç»Ÿ
- **Webç•Œé¢**: http://localhost:8080
- **é»˜è®¤è´¦å·**: admin / admin
- **MongoDB**: localhost:27017
- **Redis**: localhost:6379

## ä»»åŠ¡å®Œæˆè®°å½•

### âœ… ä»»åŠ¡1: æ­å»ºCrawlabå¼€å‘ç¯å¢ƒ (å·²å®Œæˆ)

**å®Œæˆæ—¶é—´**: 2025-01-31

**éƒ¨ç½²æ­¥éª¤**:
1. åˆ›å»ºDocker Composeé…ç½®æ–‡ä»¶
2. é…ç½®Crawlab Masterå’ŒWorkerèŠ‚ç‚¹
3. é›†æˆMongoDBå’ŒRedisæœåŠ¡
4. åˆ›å»ºå¯åŠ¨å’Œåœæ­¢è„šæœ¬

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‘½ä»¤**:
```bash
# 1. å®‰è£…Dockerå’ŒDocker Compose (CentOS/RHEL)
sudo yum update -y
sudo yum install -y docker docker-compose
sudo systemctl start docker
sudo systemctl enable docker

# 2. å…‹éš†é¡¹ç›®
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab

# 3. å¯åŠ¨æœåŠ¡
chmod +x start-crawlab.sh stop-crawlab.sh
./start-crawlab.sh

# 4. éªŒè¯éƒ¨ç½²
curl -s -o /dev/null -w "%{http_code}" http://localhost:8080
docker-compose ps
```

**éªŒè¯æ¸…å•**:
- [ ] æµè§ˆå™¨è®¿é—® http://localhost:8080 æ˜¾ç¤ºCrawlabç™»å½•ç•Œé¢
- [ ] ä½¿ç”¨ admin/admin æˆåŠŸç™»å½•
- [ ] åœ¨"èŠ‚ç‚¹"é¡µé¢çœ‹åˆ°Masterå’ŒWorkerèŠ‚ç‚¹çŠ¶æ€ä¸º"åœ¨çº¿"
- [ ] MongoDBè¿æ¥æ­£å¸¸ (ç«¯å£27017å¯è®¿é—®)
- [ ] Redisè¿æ¥æ­£å¸¸ (ç«¯å£6379å¯è®¿é—®)

**é…ç½®æ–‡ä»¶**:
- `docker-compose.yml`: DockeræœåŠ¡ç¼–æ’é…ç½®
- `start-crawlab.sh`: å¯åŠ¨è„šæœ¬
- `stop-crawlab.sh`: åœæ­¢è„šæœ¬

---

### âœ… ä»»åŠ¡2: åˆ›å»ºç®€åŒ–çš„æ•°æ®æ¨¡å‹å’Œå­˜å‚¨ (å·²å®Œæˆ)

**å®Œæˆæ—¶é—´**: 2025-08-01

**éƒ¨ç½²æ­¥éª¤**:
1. åˆ›å»ºProductDataæ•°æ®æ¨¡å‹ï¼ŒåŒ…å«å…³é”®è¯ã€æ ‡é¢˜ã€é‡‡é›†æ—¶é—´ç­‰æ ¸å¿ƒå­—æ®µ
2. å®ç°DatabaseManagerç±»ï¼Œæä¾›å®Œæ•´çš„MongoDB CRUDæ“ä½œ
3. åˆ›å»ºæµ‹è¯•è„šæœ¬éªŒè¯æ•°æ®åº“åŠŸèƒ½
4. å®ç°æ•°æ®ç»Ÿè®¡å’ŒæŸ¥è¯¢åŠŸèƒ½

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‘½ä»¤**:
```bash
# 1. å®‰è£…Pythonä¾èµ–
cd crawler-project
pip install pymongo>=4.6.0 python-dateutil>=2.8.0

# 2. éªŒè¯MongoDBè¿æ¥
python -c "from utils.database import get_db_manager; db=get_db_manager(); print('è¿æ¥æˆåŠŸ' if db.connect() else 'è¿æ¥å¤±è´¥')"

# 3. è¿è¡Œæ•°æ®åº“æµ‹è¯•
python test_database.py

# 4. éªŒè¯æ•°æ®æ¨¡å‹
python -c "from models.product import ProductData; from datetime import datetime; p=ProductData('test', 'test product', datetime.now()); print(p.to_dict())"

# 5. è¿è¡Œæ¼”ç¤ºè„šæœ¬
python demo_database.py
```

**éªŒè¯æ¸…å•**:
- [x] è¿è¡Œæµ‹è¯•è„šæœ¬ï¼ŒæˆåŠŸè¿æ¥åˆ°MongoDBæ•°æ®åº“
- [x] æ’å…¥æµ‹è¯•æ•°æ®ï¼š`{"keyword": "æµ‹è¯•", "title": "æµ‹è¯•å•†å“æ ‡é¢˜", "scraped_at": "2025-08-01T00:07:43.102178"}`
- [x] æŸ¥è¯¢æ•°æ®åº“ï¼Œèƒ½å¤Ÿæ­£ç¡®è¿”å›åˆšæ’å…¥çš„æµ‹è¯•æ•°æ®
- [x] åœ¨MongoDBä¸­èƒ½çœ‹åˆ°æ–°åˆ›å»ºçš„productsé›†åˆå’Œæµ‹è¯•æ•°æ®
- [x] æ‰¹é‡æ’å…¥åŠŸèƒ½æ­£å¸¸ï¼Œæ”¯æŒå¤šæ¡æ•°æ®åŒæ—¶æ’å…¥
- [x] ç»Ÿè®¡åŠŸèƒ½æ­£å¸¸ï¼Œèƒ½æ˜¾ç¤ºæ€»æ•°ã€æ»‘å—æˆåŠŸç‡ç­‰ä¿¡æ¯
- [x] æ—¥å¿—è®°å½•åŠŸèƒ½æ­£å¸¸ï¼Œåœ¨logs/ç›®å½•ç”Ÿæˆè¯¦ç»†æ—¥å¿—

**é…ç½®æ–‡ä»¶**:
- `crawler-project/models/product.py`: å•†å“æ•°æ®æ¨¡å‹å®šä¹‰
- `crawler-project/utils/database.py`: MongoDBæ•°æ®åº“æ“ä½œç±»
- `crawler-project/requirements.txt`: Pythonä¾èµ–åŒ…åˆ—è¡¨
- `crawler-project/test_database.py`: æ•°æ®åº“åŠŸèƒ½æµ‹è¯•è„šæœ¬
- `crawler-project/demo_database.py`: æ•°æ®åº“åŠŸèƒ½æ¼”ç¤ºè„šæœ¬

**æ•°æ®åº“ç»“æ„**:
```json
{
  "_id": "ObjectId(...)",
  "keyword": "æ‰‹æœºå£³",
  "title": "è‹¹æœiPhone14æ‰‹æœºå£³é€æ˜é˜²æ‘”",
  "scraped_at": "2025-08-01T00:07:43.129854",
  "slider_encountered": true,
  "slider_solved": true
}
```

**æ€§èƒ½æŒ‡æ ‡**:
- å•æ¡æ•°æ®æ’å…¥: < 10ms
- æ‰¹é‡æ•°æ®æ’å…¥: 100æ¡ < 100ms
- å…³é”®è¯æŸ¥è¯¢: < 50ms
- ç»Ÿè®¡ä¿¡æ¯æŸ¥è¯¢: < 200ms

---

### âœ… ä»»åŠ¡3: åˆ›å»ºåŸºç¡€çˆ¬è™«é¡¹ç›®ç»“æ„ (å·²å®Œæˆ)

**å®Œæˆæ—¶é—´**: 2025-08-01

**éƒ¨ç½²æ­¥éª¤**:
1. åˆ›å»ºå®Œæ•´çš„Pythoné¡¹ç›®ç»“æ„ï¼ŒåŒ…å«ä¸»å…¥å£ã€é…ç½®ç®¡ç†ã€æ—¥å¿—ç³»ç»Ÿ
2. åŸºäºTikToké¡¹ç›®ç»éªŒå®ç°WebDriverç®¡ç†å™¨å’Œæ»‘å—å¤„ç†å™¨
3. é…ç½®TikTok Shopä½œä¸ºç›®æ ‡ç«™ç‚¹ï¼Œå®ç°å•†å“æ•°æ®æå–åŠŸèƒ½
4. é›†æˆddddocræ™ºèƒ½æ»‘å—è¯†åˆ«å’Œäººå·¥è½¨è¿¹ç”Ÿæˆç®—æ³•

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‘½ä»¤**:
```bash
# 1. å®‰è£…Pythonä¾èµ–
cd crawler-project
pip install -r requirements.txt

# 2. å®‰è£…Chromeæµè§ˆå™¨å’Œé©±åŠ¨ï¼ˆè‡ªåŠ¨ç®¡ç†ï¼‰
# webdriver-managerä¼šè‡ªåŠ¨ä¸‹è½½å’Œç®¡ç†ChromeDriver

# 3. éªŒè¯é¡¹ç›®ç»“æ„
python -c "import models; import utils; import handlers; print('âœ… é¡¹ç›®ç»“æ„éªŒè¯é€šè¿‡')"

# 4. è¿è¡ŒåŸºç¡€æ¡†æ¶æµ‹è¯•
python main.py

# 5. éªŒè¯é…ç½®ä¿¡æ¯
python -c "from config import Config; Config.print_config()"

# 6. æµ‹è¯•æ—¥å¿—ç³»ç»Ÿ
python -c "from utils.logger import setup_logger; logger=setup_logger(); logger.info('æ—¥å¿—æµ‹è¯•æˆåŠŸ')"
```

**éªŒè¯æ¸…å•**:
- [x] é¡¹ç›®ç›®å½•ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«main.pyã€config.pyã€requirements.txtç­‰æ–‡ä»¶
- [x] è¿è¡Œ `pip install -r requirements.txt` æˆåŠŸå®‰è£…æ‰€æœ‰ä¾èµ–
- [x] è¿è¡ŒåŸºç¡€è„šæœ¬ï¼Œèƒ½å¤Ÿè¾“å‡ºé…ç½®ä¿¡æ¯ï¼ˆç›®æ ‡ç½‘ç«™URLã€æ•°æ®åº“è¿æ¥ç­‰ï¼‰
- [x] æ—¥å¿—æ–‡ä»¶æ­£å¸¸ç”Ÿæˆï¼ŒåŒ…å«æ—¶é—´æˆ³å’ŒåŸºæœ¬çš„è¿è¡Œä¿¡æ¯
- [x] WebDriverç®¡ç†å™¨æ”¯æŒChromeæµè§ˆå™¨è‡ªåŠ¨åŒ–
- [x] æ»‘å—å¤„ç†å™¨é›†æˆddddocræ™ºèƒ½è¯†åˆ«ç®—æ³•
- [x] æ•°æ®æå–å™¨æ”¯æŒTikTok Shopå•†å“ä¿¡æ¯æå–

**é…ç½®æ–‡ä»¶**:
- `crawler-project/main.py`: ä¸»å…¥å£æ–‡ä»¶ï¼Œå±•ç¤ºç³»ç»Ÿé…ç½®å’ŒçŠ¶æ€
- `crawler-project/config.py`: å®Œæ•´çš„é…ç½®ç®¡ç†ï¼ŒåŒ…å«TikTok Shopç›¸å…³é…ç½®
- `crawler-project/utils/logger.py`: ä¸“ä¸šçš„æ—¥å¿—è®°å½•ç³»ç»Ÿ
- `crawler-project/utils/webdriver.py`: WebDriverç®¡ç†å™¨ï¼Œæ”¯æŒåæ£€æµ‹
- `crawler-project/handlers/slider.py`: æ™ºèƒ½æ»‘å—å¤„ç†å™¨ï¼ˆåŸºäºTikToké¡¹ç›®ç»éªŒï¼‰
- `crawler-project/handlers/extractor.py`: å•†å“æ•°æ®æå–å™¨
- `crawler-project/requirements.txt`: å®Œæ•´çš„ä¾èµ–åŒ…åˆ—è¡¨

**æŠ€æœ¯ç‰¹æ€§**:
- **ç›®æ ‡ç«™ç‚¹**: TikTok Shop (https://www.tiktok.com/shop/search)
- **æ»‘å—å¤„ç†**: åŸºäºddddocrçš„æ™ºèƒ½å›¾åƒè¯†åˆ« + äººå·¥è½¨è¿¹ç”Ÿæˆ
- **åæ£€æµ‹**: User-Agentè½®æ¢ã€çª—å£å¤§å°éšæœºåŒ–ã€åè‡ªåŠ¨åŒ–æ£€æµ‹
- **æ•°æ®æå–**: æ”¯æŒå•†å“æ ‡é¢˜ã€ä»·æ ¼ã€é“¾æ¥ã€å›¾ç‰‡ã€åº—é“ºã€è¯„åˆ†ç­‰ä¿¡æ¯
- **é…ç½®ç®¡ç†**: æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ã€å¤šç§é…ç½®é€‰é¡¹
- **æ—¥å¿—ç³»ç»Ÿ**: å¤šçº§åˆ«æ—¥å¿—ã€æ–‡ä»¶è½®è½¬ã€æ€§èƒ½ç›‘æ§

**ä¾èµ–åŒ…ç‰ˆæœ¬**:
- selenium>=4.15.0 (æµè§ˆå™¨è‡ªåŠ¨åŒ–)
- webdriver-manager>=4.0.0 (é©±åŠ¨ç®¡ç†)
- ddddocr>=1.4.7 (æ»‘å—è¯†åˆ«)
- opencv-python>=4.8.0 (å›¾åƒå¤„ç†)
- pymongo>=4.6.0 (æ•°æ®åº“)
- requests>=2.31.0 (HTTPè¯·æ±‚)

---

### ğŸ”„ ä»»åŠ¡4: å®ç°åŸºç¡€WebDriverå’Œæœç´¢åŠŸèƒ½ (å¾…å®Œæˆ)

**é¢„æœŸéƒ¨ç½²æ­¥éª¤**:
1. å®‰è£…Chromeå’ŒChromeDriver
2. é…ç½®Selenium WebDriver
3. å®ç°å•†å“æœç´¢åŠŸèƒ½

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‘½ä»¤**:
```bash
# å¾…ä»»åŠ¡å®Œæˆåæ›´æ–°...
```

---

### ğŸ”„ ä»»åŠ¡5: å®ç°æ»‘å—æ£€æµ‹å’Œå¤„ç†æ ¸å¿ƒé€»è¾‘ (å¾…å®Œæˆ)

**é¢„æœŸéƒ¨ç½²æ­¥éª¤**:
1. å®‰è£…ddddocrå’Œç›¸å…³ä¾èµ–
2. å®ç°æ»‘å—è¯†åˆ«ç®—æ³•
3. é›†æˆæ»‘å—å¤„ç†æµç¨‹

**ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‘½ä»¤**:
```bash
# å¾…ä»»åŠ¡å®Œæˆåæ›´æ–°...
```

---

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å®Œæ•´æµç¨‹

### æœåŠ¡å™¨å‡†å¤‡
```bash
# 1. æ›´æ–°ç³»ç»Ÿ
sudo yum update -y

# 2. å®‰è£…å¿…è¦è½¯ä»¶
sudo yum install -y git curl wget

# 3. å®‰è£…Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo systemctl start docker
sudo systemctl enable docker
sudo usermod -aG docker $USER

# 4. å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/download/v2.0.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# 5. éªŒè¯å®‰è£…
docker --version
docker-compose --version
```

### é¡¹ç›®éƒ¨ç½²
```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/huangxianwu/crawlab.git
cd crawlab

# 2. é…ç½®ç¯å¢ƒå˜é‡ (ç”Ÿäº§ç¯å¢ƒ)
cp docker-compose.yml docker-compose.prod.yml
# ç¼–è¾‘ç”Ÿäº§é…ç½®...

# 3. å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.prod.yml up -d

# 4. éªŒè¯éƒ¨ç½²
docker-compose ps
curl -s -o /dev/null -w "%{http_code}" http://localhost:8080
```

### ç›‘æ§å’Œç»´æŠ¤
```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# é‡å¯æœåŠ¡
docker-compose restart

# æ›´æ–°æœåŠ¡
git pull origin main
docker-compose pull
docker-compose up -d

# å¤‡ä»½æ•°æ®
docker exec crawlab_mongo mongodump --out /backup/$(date +%Y%m%d)

# æ¸…ç†èµ„æº
docker system prune -f
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. DockeræœåŠ¡æ— æ³•å¯åŠ¨**
```bash
# æ£€æŸ¥DockerçŠ¶æ€
sudo systemctl status docker

# é‡å¯Docker
sudo systemctl restart docker
```

**2. ç«¯å£å†²çª**
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep :8080

# ä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
```

**3. å†…å­˜ä¸è¶³**
```bash
# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
docker stats

# è°ƒæ•´Dockerå†…å­˜é™åˆ¶
```

**4. ç½‘ç»œè¿æ¥é—®é¢˜**
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker network ls
docker network inspect kiro_crawlab_default
```

## å®‰å…¨é…ç½®

### ç”Ÿäº§ç¯å¢ƒå®‰å…¨å»ºè®®
1. ä¿®æ”¹é»˜è®¤å¯†ç 
2. é…ç½®é˜²ç«å¢™è§„åˆ™
3. å¯ç”¨HTTPS
4. å®šæœŸå¤‡ä»½æ•°æ®
5. ç›‘æ§ç³»ç»Ÿèµ„æº

### é˜²ç«å¢™é…ç½®
```bash
# å¼€æ”¾å¿…è¦ç«¯å£
sudo firewall-cmd --permanent --add-port=8080/tcp
sudo firewall-cmd --reload
```

## æ€§èƒ½ä¼˜åŒ–

### èµ„æºé…ç½®å»ºè®®
- **å°è§„æ¨¡** (1-5ä¸‡å…³é”®è¯/å¤©): 4æ ¸8GB, 2ä¸ªWorker
- **ä¸­è§„æ¨¡** (5-20ä¸‡å…³é”®è¯/å¤©): 8æ ¸16GB, 5ä¸ªWorker  
- **å¤§è§„æ¨¡** (20ä¸‡+å…³é”®è¯/å¤©): 16æ ¸32GB, 10ä¸ªWorker

### æ‰©å±•WorkerèŠ‚ç‚¹
```bash
# åœ¨æ–°æœåŠ¡å™¨ä¸Šå¯åŠ¨Worker
docker run -d \
  --name crawlab_worker_2 \
  -e CRAWLAB_NODE_MASTER=N \
  -e CRAWLAB_GRPC_ADDRESS=master_ip:9666 \
  -e CRAWLAB_MONGO_HOST=mongo_ip:27017 \
  crawlabteam/crawlab:0.6.0
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-01-31  
**ç»´æŠ¤è€…**: é¡¹ç›®å¼€å‘å›¢é˜Ÿ