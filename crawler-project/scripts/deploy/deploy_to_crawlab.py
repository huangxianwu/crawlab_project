#!/usr/bin/env python3
"""
Crawlabéƒ¨ç½²è„šæœ¬
å°†çˆ¬è™«é¡¹ç›®éƒ¨ç½²åˆ°Crawlabå¹³å°
"""
import os
import sys
import json
import shutil
import zipfile
from pathlib import Path


def create_crawlab_package():
    """åˆ›å»ºCrawlabçˆ¬è™«åŒ…"""
    print("ğŸš€ åˆ›å»ºCrawlabçˆ¬è™«åŒ…...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    package_dir = Path("crawlab_package")
    if package_dir.exists():
        shutil.rmtree(package_dir)
    package_dir.mkdir()
    
    # éœ€è¦åŒ…å«çš„æ–‡ä»¶å’Œç›®å½•
    include_files = [
        "crawlab_spider.py",
        "spider.json",
        "requirements.txt",
        "config.py",
        "models/",
        "utils/",
        "handlers/",
        "README.md"
    ]
    
    # å¤åˆ¶æ–‡ä»¶åˆ°åŒ…ç›®å½•
    for item in include_files:
        src_path = Path(item)
        if src_path.exists():
            if src_path.is_file():
                shutil.copy2(src_path, package_dir / src_path.name)
                print(f"âœ… å¤åˆ¶æ–‡ä»¶: {item}")
            elif src_path.is_dir():
                shutil.copytree(src_path, package_dir / src_path.name)
                print(f"âœ… å¤åˆ¶ç›®å½•: {item}")
        else:
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {item}")
    
    # åˆ›å»ºZIPåŒ…
    zip_path = Path("ecommerce_crawler.zip")
    if zip_path.exists():
        zip_path.unlink()
    
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(package_dir):
            for file in files:
                file_path = Path(root) / file
                arc_path = file_path.relative_to(package_dir)
                zipf.write(file_path, arc_path)
    
    # æ¸…ç†ä¸´æ—¶ç›®å½•
    shutil.rmtree(package_dir)
    
    print(f"âœ… Crawlabçˆ¬è™«åŒ…åˆ›å»ºå®Œæˆ: {zip_path}")
    return zip_path


def validate_spider_config():
    """éªŒè¯çˆ¬è™«é…ç½®"""
    print("\nğŸ” éªŒè¯çˆ¬è™«é…ç½®...")
    
    # æ£€æŸ¥spider.json
    spider_json_path = Path("spider.json")
    if not spider_json_path.exists():
        print("âŒ spider.jsonæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        with open(spider_json_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ['name', 'cmd', 'params']
        for field in required_fields:
            if field not in config:
                print(f"âŒ spider.jsonç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                return False
        
        print("âœ… spider.jsoné…ç½®éªŒè¯é€šè¿‡")
        
        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"  çˆ¬è™«åç§°: {config['name']}")
        print(f"  æ‰§è¡Œå‘½ä»¤: {config['cmd']}")
        print(f"  å‚æ•°æ•°é‡: {len(config.get('params', []))}")
        
        return True
        
    except json.JSONDecodeError as e:
        print(f"âŒ spider.jsonæ ¼å¼é”™è¯¯: {e}")
        return False


def test_spider_locally():
    """æœ¬åœ°æµ‹è¯•çˆ¬è™«"""
    print("\nğŸ§ª æœ¬åœ°æµ‹è¯•çˆ¬è™«...")
    
    try:
        # æµ‹è¯•å¯¼å…¥
        sys.path.append('.')
        
        print("æµ‹è¯•æ¨¡å—å¯¼å…¥...")
        from config import Config
        from utils.logger import setup_logger
        from models.product import ProductData
        print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•çˆ¬è™«è„šæœ¬è¯­æ³•
        print("æµ‹è¯•çˆ¬è™«è„šæœ¬è¯­æ³•...")
        with open('crawlab_spider.py', 'r', encoding='utf-8') as f:
            spider_code = f.read()
        
        compile(spider_code, 'crawlab_spider.py', 'exec')
        print("âœ… çˆ¬è™«è„šæœ¬è¯­æ³•æ£€æŸ¥é€šè¿‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœ¬åœ°æµ‹è¯•å¤±è´¥: {e}")
        return False


def generate_deployment_guide():
    """ç”Ÿæˆéƒ¨ç½²æŒ‡å—"""
    print("\nğŸ“ ç”Ÿæˆéƒ¨ç½²æŒ‡å—...")
    
    guide_content = """# Crawlabç”µå•†çˆ¬è™«éƒ¨ç½²æŒ‡å—

## 1. å‡†å¤‡å·¥ä½œ

### 1.1 ç¡®ä¿Crawlabç¯å¢ƒè¿è¡Œæ­£å¸¸
```bash
# æ£€æŸ¥CrawlabæœåŠ¡çŠ¶æ€
docker-compose ps

# è®¿é—®Crawlab Webç•Œé¢
# http://localhost:8080
```

### 1.2 å‡†å¤‡çˆ¬è™«åŒ…
- çˆ¬è™«åŒ…æ–‡ä»¶: `ecommerce_crawler.zip`
- åŒ…å«æ‰€æœ‰å¿…è¦çš„ä»£ç å’Œé…ç½®æ–‡ä»¶

## 2. éƒ¨ç½²æ­¥éª¤

### 2.1 ä¸Šä¼ çˆ¬è™«
1. ç™»å½•Crawlab Webç•Œé¢ (http://localhost:8080)
2. è¿›å…¥"çˆ¬è™«"é¡µé¢
3. ç‚¹å‡»"æ–°å»ºçˆ¬è™«"
4. é€‰æ‹©"ä¸Šä¼ ZIPæ–‡ä»¶"
5. ä¸Šä¼  `ecommerce_crawler.zip`
6. ç­‰å¾…ä¸Šä¼ å’Œè§£æå®Œæˆ

### 2.2 é…ç½®çˆ¬è™«
1. çˆ¬è™«åç§°: "ç”µå•†çˆ¬è™«"
2. æ‰§è¡Œå‘½ä»¤: `python crawlab_spider.py`
3. é…ç½®å‚æ•°:
   - keywords: æœç´¢å…³é”®è¯ (é»˜è®¤: phone case,wireless charger)
   - max_pages: æœ€å¤§é¡µæ•° (é»˜è®¤: 1)

### 2.3 ç¯å¢ƒå˜é‡é…ç½®
- MONGO_URI: mongodb://mongo:27017
- DATABASE_NAME: crawler_db
- LOG_LEVEL: INFO

## 3. è¿è¡Œæµ‹è¯•

### 3.1 åˆ›å»ºä»»åŠ¡
1. è¿›å…¥çˆ¬è™«è¯¦æƒ…é¡µ
2. ç‚¹å‡»"è¿è¡Œ"æŒ‰é’®
3. è®¾ç½®å‚æ•°:
   - keywords: "æ•°æ®çº¿"
   - max_pages: 1
4. ç‚¹å‡»"å¼€å§‹"

### 3.2 ç›‘æ§æ‰§è¡Œ
1. åœ¨"ä»»åŠ¡"é¡µé¢æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
2. çŠ¶æ€å˜åŒ–: ç­‰å¾…ä¸­ â†’ è¿è¡Œä¸­ â†’ æˆåŠŸ
3. ç‚¹å‡»ä»»åŠ¡è¯¦æƒ…æŸ¥çœ‹æ—¥å¿—

### 3.3 æŸ¥çœ‹ç»“æœ
1. åœ¨ä»»åŠ¡è¯¦æƒ…é¡µæŸ¥çœ‹é‡‡é›†ç»Ÿè®¡
2. åœ¨"ç»“æœ"é¡µé¢æŸ¥çœ‹é‡‡é›†çš„å•†å“æ•°æ®
3. æ£€æŸ¥MongoDBæ•°æ®åº“ä¸­çš„æ•°æ®

## 4. éªŒè¯æ ‡å‡†

- âœ… åœ¨Crawlab Webç•Œé¢èƒ½çœ‹åˆ°"ç”µå•†çˆ¬è™«"é¡¹ç›®
- âœ… èƒ½å¤ŸæˆåŠŸåˆ›å»ºå’Œè¿è¡Œä»»åŠ¡
- âœ… ä»»åŠ¡çŠ¶æ€æ­£å¸¸å˜åŒ–: ç­‰å¾…ä¸­â†’è¿è¡Œä¸­â†’æˆåŠŸ
- âœ… èƒ½çœ‹åˆ°å®Œæ•´çš„æ‰§è¡Œæ—¥å¿—å’Œé‡‡é›†ç»“æœç»Ÿè®¡
- âœ… æ•°æ®æ­£ç¡®ä¿å­˜åˆ°MongoDBæ•°æ®åº“

## 5. æ•…éšœæ’é™¤

### 5.1 å¸¸è§é—®é¢˜
- ä¾èµ–å®‰è£…å¤±è´¥: æ£€æŸ¥requirements.txt
- WebDriveré”™è¯¯: ç¡®ä¿Chromeå·²å®‰è£…
- æ•°æ®åº“è¿æ¥å¤±è´¥: æ£€æŸ¥MongoDBæœåŠ¡çŠ¶æ€
- æ»‘å—å¤„ç†å¤±è´¥: æ£€æŸ¥ddddocrä¾èµ–

### 5.2 æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹Crawlabå®¹å™¨æ—¥å¿—
docker-compose logs -f master
docker-compose logs -f worker

# æŸ¥çœ‹çˆ¬è™«æ‰§è¡Œæ—¥å¿—
# åœ¨Crawlab Webç•Œé¢çš„ä»»åŠ¡è¯¦æƒ…é¡µæŸ¥çœ‹
```

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 èµ„æºé…ç½®
- å†…å­˜: å»ºè®®è‡³å°‘1GB
- CPU: å»ºè®®è‡³å°‘2æ ¸
- ç£ç›˜: å»ºè®®è‡³å°‘500MB

### 6.2 å¹¶å‘é…ç½®
- å•èŠ‚ç‚¹å»ºè®®æœ€å¤š2ä¸ªå¹¶å‘ä»»åŠ¡
- å¯é€šè¿‡å¢åŠ WorkerèŠ‚ç‚¹æé«˜å¹¶å‘èƒ½åŠ›

## 7. ç›‘æ§å’Œç»´æŠ¤

### 7.1 å®šæœŸæ£€æŸ¥
- æ¯æ—¥æ£€æŸ¥ä»»åŠ¡æ‰§è¡Œæƒ…å†µ
- ç›‘æ§æ•°æ®åº“å­˜å‚¨ç©ºé—´
- æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°

### 7.2 æ•°æ®å¤‡ä»½
```bash
# å¤‡ä»½MongoDBæ•°æ®
docker exec crawlab_mongo mongodump --out /backup/$(date +%Y%m%d)
```
"""
    
    with open('CRAWLAB_DEPLOYMENT_GUIDE.md', 'w', encoding='utf-8') as f:
        f.write(guide_content)
    
    print("âœ… éƒ¨ç½²æŒ‡å—å·²ç”Ÿæˆ: CRAWLAB_DEPLOYMENT_GUIDE.md")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Crawlabç”µå•†çˆ¬è™«éƒ¨ç½²å·¥å…·")
    print("=" * 50)
    
    # éªŒè¯é…ç½®
    if not validate_spider_config():
        print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥spider.json")
        return False
    
    # æœ¬åœ°æµ‹è¯•
    if not test_spider_locally():
        print("âŒ æœ¬åœ°æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        return False
    
    # åˆ›å»ºéƒ¨ç½²åŒ…
    zip_path = create_crawlab_package()
    
    # ç”Ÿæˆéƒ¨ç½²æŒ‡å—
    generate_deployment_guide()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Crawlabçˆ¬è™«éƒ¨ç½²å‡†å¤‡å®Œæˆï¼")
    print("=" * 50)
    print(f"ğŸ“¦ çˆ¬è™«åŒ…: {zip_path}")
    print(f"ğŸ“ éƒ¨ç½²æŒ‡å—: CRAWLAB_DEPLOYMENT_GUIDE.md")
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
    print("1. ç¡®ä¿CrawlabæœåŠ¡æ­£åœ¨è¿è¡Œ")
    print("2. è®¿é—® http://localhost:8080")
    print("3. ä¸Šä¼ çˆ¬è™«åŒ…å¹¶é…ç½®å‚æ•°")
    print("4. è¿è¡Œæµ‹è¯•ä»»åŠ¡éªŒè¯åŠŸèƒ½")
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)