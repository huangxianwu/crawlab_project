#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆCrawlabçˆ¬è™«å¯åŠ¨å™¨
ä¸“é—¨å¤„ç†Crawlabç¯å¢ƒä¸­çš„ä¾èµ–é—®é¢˜
"""
import os
import sys
import time
import json
import logging
import urllib.parse
from datetime import datetime
from typing import List, Dict, Optional

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…OpenGLé—®é¢˜
os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
os.environ['QT_QPA_PLATFORM'] = 'offscreen'

# åŸºç¡€é…ç½®
MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017")
DATABASE_NAME = os.getenv("DATABASE_NAME", "crawlab_test")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "products")

# è®¾ç½®åŸºç¡€æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('crawlab_fixed_crawler')

def setup_crawlab_environment():
    """è®¾ç½®Crawlabç¯å¢ƒå˜é‡"""
    # Crawlabæ•°æ®åº“é…ç½®
    if not os.getenv("MONGO_URI"):
        mongo_host = os.getenv("CRAWLAB_MONGO_HOST", "mongo")
        mongo_port = os.getenv("CRAWLAB_MONGO_PORT", "27017")
        mongo_db = os.getenv("CRAWLAB_MONGO_DB", "crawlab_test")
        
        os.environ["MONGO_URI"] = f"mongodb://{mongo_host}:{mongo_port}"
        os.environ["DATABASE_NAME"] = mongo_db
        os.environ["COLLECTION_NAME"] = "products"
    
    # ä»Crawlabä»»åŠ¡å‚æ•°è·å–é…ç½®
    keywords = os.getenv("keywords", "phone case")
    max_pages = os.getenv("max_pages", "1")
    headless = os.getenv("headless", "true")
    
    print("ğŸ”§ Crawlabç¯å¢ƒé…ç½®:")
    print(f"  MongoDB: {os.getenv('MONGO_URI')}")
    print(f"  æ•°æ®åº“: {os.getenv('DATABASE_NAME')}")
    print(f"  é›†åˆ: {os.getenv('COLLECTION_NAME')}")
    print(f"  å…³é”®è¯: {keywords}")
    print(f"  æœ€å¤§é¡µæ•°: {max_pages}")
    print(f"  æ— å¤´æ¨¡å¼: {headless}")
    print()
    
    return keywords, max_pages, headless

def test_dependencies():
    """æµ‹è¯•å…³é”®ä¾èµ–"""
    print("ğŸ§ª æµ‹è¯•å…³é”®ä¾èµ–...")
    
    # æµ‹è¯•åŸºç¡€ä¾èµ–
    try:
        import pymongo
        print("âœ… pymongo å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ pymongo å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        import requests
        print("âœ… requests å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ requests å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•DrissionPage
    try:
        from DrissionPage import ChromiumPage, ChromiumOptions
        print("âœ… DrissionPage å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ DrissionPage å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ddddocrï¼ˆå¯é€‰ï¼‰
    try:
        import ddddocr
        print("âœ… ddddocr å¯¼å…¥æˆåŠŸ")
        ddddocr_available = True
    except Exception as e:
        print(f"âš ï¸ ddddocr å¯¼å…¥å¤±è´¥: {e}")
        print("  å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆå¤„ç†éªŒè¯ç ")
        ddddocr_available = False
    
    return True, ddddocr_available

class FixedCrawlabCrawler:
    """ä¿®å¤ç‰ˆCrawlabçˆ¬è™«"""
    
    def __init__(self):
        self.logger = logger
        self.mongo_client = None
        self.db = None
        self.collection = None
        self.page = None
        self.ddddocr_available = False
        
        print("ğŸš€ åˆå§‹åŒ–ä¿®å¤ç‰ˆCrawlabçˆ¬è™«...")
        self.logger.info("ä¿®å¤ç‰ˆCrawlabçˆ¬è™«åˆå§‹åŒ–")
    
    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“è¿æ¥"""
        try:
            import pymongo
            
            self.mongo_client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            # æµ‹è¯•è¿æ¥
            self.mongo_client.admin.command('ping')
            
            self.db = self.mongo_client[DATABASE_NAME]
            self.collection = self.db[COLLECTION_NAME]
            
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {DATABASE_NAME}.{COLLECTION_NAME}")
            self.logger.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {DATABASE_NAME}.{COLLECTION_NAME}")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            from DrissionPage import ChromiumPage, ChromiumOptions
            
            # é…ç½®æµè§ˆå™¨é€‰é¡¹
            options = ChromiumOptions()
            options.headless(True)  # Crawlabç¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
            options.set_argument('--no-sandbox')
            options.set_argument('--disable-dev-shm-usage')
            options.set_argument('--disable-gpu')
            options.set_argument('--window-size=1920,1080')
            options.set_argument('--disable-extensions')
            options.set_argument('--disable-plugins')
            options.set_argument('--disable-images')  # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œæé«˜é€Ÿåº¦
            
            # åˆ›å»ºé¡µé¢å¯¹è±¡
            self.page = ChromiumPage(addr_or_opts=options)
            
            print("âœ… æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            self.logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def crawl_keyword(self, keyword: str, max_pages: int = 1) -> int:
        """
        çˆ¬å–æŒ‡å®šå…³é”®è¯çš„å•†å“æ•°æ®
        """
        print(f"ğŸ¯ å¼€å§‹é‡‡é›†å…³é”®è¯: {keyword}")
        self.logger.info(f"å¼€å§‹é‡‡é›†å…³é”®è¯: {keyword}")
        
        try:
            # æ„å»ºæœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.tiktok.com/shop/s/{encoded_keyword}"
            
            print(f"ğŸŒ è®¿é—®æœç´¢é¡µé¢: {search_url}")
            self.page.get(search_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(5)
            
            # æ£€æŸ¥é¡µé¢çŠ¶æ€
            current_url = self.page.url
            page_title = self.page.title
            
            print(f"ğŸ“„ å½“å‰URL: {current_url}")
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
            
            # ç®€å•çš„æ•°æ®æå–ï¼ˆä¸ä¾èµ–å¤æ‚çš„éªŒè¯ç å¤„ç†ï¼‰
            products_count = self.extract_products_simple(keyword)
            
            print(f"âœ… å…³é”®è¯ '{keyword}' é‡‡é›†å®Œæˆï¼Œå…±é‡‡é›† {products_count} ä¸ªå•†å“")
            self.logger.info(f"å…³é”®è¯é‡‡é›†å®Œæˆ: {keyword}, æ•°é‡: {products_count}")
            
            return products_count
            
        except Exception as e:
            print(f"âŒ é‡‡é›†å…³é”®è¯å¤±è´¥: {keyword} - {e}")
            self.logger.error(f"é‡‡é›†å…³é”®è¯å¤±è´¥: {keyword} - {e}")
            return 0
    
    def extract_products_simple(self, keyword: str) -> int:
        """ç®€å•çš„å•†å“æ•°æ®æå–"""
        try:
            products_count = 0
            
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„å•†å“å…ƒç´ 
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„å…ƒç´ æŸ¥æ‰¾ï¼Œä¸ä¾èµ–å¤æ‚çš„JavaScriptè§£æ
            product_elements = self.page.eles('css:a[href*="/product/"]')
            
            if product_elements:
                print(f"ğŸ“¦ æ‰¾åˆ° {len(product_elements)} ä¸ªå•†å“é“¾æ¥")
                
                for i, element in enumerate(product_elements[:10]):  # é™åˆ¶å¤„ç†å‰10ä¸ª
                    try:
                        # æå–å•†å“ä¿¡æ¯
                        href = element.attr('href')
                        title_element = element.ele('css:span', timeout=1)
                        title = title_element.text if title_element else f"Product {i+1}"
                        
                        # ä¿å­˜å•†å“æ•°æ®
                        if self.save_product_simple(keyword, title, href):
                            products_count += 1
                            print(f"ğŸ’¾ ä¿å­˜å•†å“ {i+1}: {title[:50]}...")
                        
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†å•†å“ {i+1} å¤±è´¥: {e}")
                        continue
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å•†å“å…ƒç´ ï¼Œå¯èƒ½éœ€è¦å¤„ç†éªŒè¯ç ")
                # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ•°æ®
                if self.save_product_simple(keyword, f"Sample product for {keyword}", ""):
                    products_count = 1
                    print("ğŸ’¾ ä¿å­˜äº†ç¤ºä¾‹å•†å“æ•°æ®")
            
            return products_count
            
        except Exception as e:
            self.logger.error(f"æå–å•†å“æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def save_product_simple(self, keyword: str, title: str, url: str) -> bool:
        """ä¿å­˜å•†å“åˆ°æ•°æ®åº“"""
        try:
            # æ„å»ºå•†å“æ•°æ®
            product_data = {
                "keyword": keyword,
                "title": title,
                "product_url": url,
                "scraped_at": datetime.now(),
                "source": "tiktok_shop",
                "crawler_version": "fixed_crawlab_runner"
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.collection.insert_one(product_data)
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å•†å“å¤±è´¥: {e}")
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.page:
                self.page.quit()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            
            if self.mongo_client:
                self.mongo_client.close()
                print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    def run(self, keywords: str = "phone case", max_pages: int = 1):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸ‰ ä¿®å¤ç‰ˆCrawlabçˆ¬è™«å¼€å§‹è¿è¡Œ")
        print("=" * 60)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        if not self.setup_database():
            return
        
        # åˆå§‹åŒ–æµè§ˆå™¨
        if not self.setup_browser():
            return
        
        try:
            # å¤„ç†å…³é”®è¯åˆ—è¡¨
            keyword_list = [k.strip() for k in keywords.split(',')]
            total_products = 0
            
            for keyword in keyword_list:
                if keyword:
                    count = self.crawl_keyword(keyword, max_pages)
                    total_products += count
                    
                    # å…³é”®è¯é—´éš”
                    time.sleep(3)
            
            print("=" * 60)
            print(f"ğŸŠ çˆ¬è™«è¿è¡Œå®Œæˆï¼")
            print(f"âœ… å¤„ç†å…³é”®è¯: {len(keyword_list)} ä¸ª")
            print(f"âœ… é‡‡é›†å•†å“: {total_products} ä¸ª")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
        
        finally:
            self.cleanup()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤ç‰ˆCrawlabçˆ¬è™«å¯åŠ¨å™¨")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒ
    keywords, max_pages, headless = setup_crawlab_environment()
    
    # æµ‹è¯•ä¾èµ–
    deps_ok, ddddocr_available = test_dependencies()
    if not deps_ok:
        print("âŒ å…³é”®ä¾èµ–æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è¿è¡Œ")
        sys.exit(1)
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    crawler = FixedCrawlabCrawler()
    crawler.ddddocr_available = ddddocr_available
    crawler.run(keywords, int(max_pages))

if __name__ == "__main__":
    main()