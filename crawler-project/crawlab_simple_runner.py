#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆCrawlabçˆ¬è™«è¿è¡Œå™¨
é¿å…å¤æ‚çš„æ¨¡å—ä¾èµ–ï¼Œç›´æ¥å†…è”æ‰€æœ‰å¿…è¦çš„åŠŸèƒ½
"""
import sys
import os
import time
import json
import logging
import urllib.parse
from datetime import datetime
from typing import List, Dict, Optional

# åŸºç¡€é…ç½®
MONGO_URI = os.getenv("MONGO_URI", "mongodb://localhost:27017")
DATABASE_NAME = os.getenv("DATABASE_NAME", "crawler_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "products")

# è®¾ç½®åŸºç¡€æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('crawlab_crawler')

def log_debug_info():
    """è¾“å‡ºè°ƒè¯•ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸ” [DEBUG] Crawlabç®€åŒ–çˆ¬è™«è°ƒè¯•ä¿¡æ¯")
    print("=" * 60)
    print(f"[DEBUG] Pythonç‰ˆæœ¬: {sys.version}")
    print(f"[DEBUG] è„šæœ¬æ–‡ä»¶: {__file__}")
    print(f"[DEBUG] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"[DEBUG] ç¯å¢ƒå˜é‡:")
    print(f"  MONGO_URI: {MONGO_URI}")
    print(f"  DATABASE_NAME: {DATABASE_NAME}")
    print(f"  COLLECTION_NAME: {COLLECTION_NAME}")
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    key_files = ['config.py', 'requirements.txt']
    for file in key_files:
        exists = os.path.exists(file)
        print(f"  {file}: {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
    
    print("=" * 60)

class SimpleCrawlabCrawler:
    """ç®€åŒ–çš„Crawlabçˆ¬è™«"""
    
    def __init__(self):
        self.logger = logger
        self.mongo_client = None
        self.db = None
        self.collection = None
        self.page = None
        self.det = None
        
        print("ğŸš€ åˆå§‹åŒ–ç®€åŒ–ç‰ˆCrawlabçˆ¬è™«...")
        self.logger.info("ç®€åŒ–ç‰ˆCrawlabçˆ¬è™«åˆå§‹åŒ–")
    
    def setup_dependencies(self):
        """è®¾ç½®ä¾èµ–"""
        try:
            # å°è¯•å¯¼å…¥å¿…è¦çš„ä¾èµ–
            global ChromiumPage, ChromiumOptions, ddddocr, pymongo
            
            from DrissionPage import ChromiumPage, ChromiumOptions
            import ddddocr
            import pymongo
            
            print("âœ… æ‰€æœ‰ä¾èµ–å¯¼å…¥æˆåŠŸ")
            self.logger.info("ä¾èµ–å¯¼å…¥æˆåŠŸ")
            return True
            
        except ImportError as e:
            print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
            self.logger.error(f"ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“è¿æ¥"""
        try:
            self.mongo_client = pymongo.MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
            # æµ‹è¯•è¿æ¥
            self.mongo_client.admin.command('ping')
            
            self.db = self.mongo_client[DATABASE_NAME]
            self.collection = self.db[COLLECTION_NAME]
            
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {DATABASE_NAME}.{COLLECTION_NAME}")
            self.logger.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {DATABASE_NAME}.{COLLECTION_NAME}")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            self.logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        try:
            # é…ç½®æµè§ˆå™¨é€‰é¡¹
            options = ChromiumOptions()
            options.headless(True)  # Crawlabç¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
            options.set_argument('--no-sandbox')
            options.set_argument('--disable-dev-shm-usage')
            options.set_argument('--disable-gpu')
            options.set_argument('--window-size=1920,1080')
            
            # åˆ›å»ºé¡µé¢å¯¹è±¡
            self.page = ChromiumPage(addr_or_opts=options)
            
            print("âœ… æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            self.logger.info("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def setup_captcha_solver(self):
        """è®¾ç½®éªŒè¯ç è¯†åˆ«"""
        try:
            self.det = ddddocr.DdddOcr(det=False, ocr=False, show_ad=False)
            print("âœ… éªŒè¯ç è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            self.logger.info("éªŒè¯ç è¯†åˆ«å™¨åˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ éªŒè¯ç è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.logger.error(f"éªŒè¯ç è¯†åˆ«å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def crawl_keyword(self, keyword: str, max_pages: int = 1) -> int:
        """
        çˆ¬å–æŒ‡å®šå…³é”®è¯çš„å•†å“æ•°æ®
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_pages: æœ€å¤§é¡µæ•°
            
        Returns:
            int: é‡‡é›†åˆ°çš„å•†å“æ•°é‡
        """
        print(f"ğŸ¯ å¼€å§‹é‡‡é›†å…³é”®è¯: {keyword}")
        self.logger.info(f"å¼€å§‹é‡‡é›†å…³é”®è¯: {keyword}")
        
        try:
            # æ„å»ºæœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.tiktok.com/shop/s/{encoded_keyword}"
            
            print(f"ğŸŒ è®¿é—®æœç´¢é¡µé¢: {search_url}")
            self.page.get(search_url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            time.sleep(3)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç 
            if self.detect_captcha():
                print("ğŸ§© æ£€æµ‹åˆ°éªŒè¯ç ï¼Œå°è¯•å¤„ç†...")
                if not self.handle_captcha():
                    print("âŒ éªŒè¯ç å¤„ç†å¤±è´¥")
                    return 0
                print("âœ… éªŒè¯ç å¤„ç†æˆåŠŸ")
            
            # æå–å•†å“æ•°æ®
            products_count = self.extract_products(keyword)
            
            print(f"âœ… å…³é”®è¯ '{keyword}' é‡‡é›†å®Œæˆï¼Œå…±é‡‡é›† {products_count} ä¸ªå•†å“")
            self.logger.info(f"å…³é”®è¯é‡‡é›†å®Œæˆ: {keyword}, æ•°é‡: {products_count}")
            
            return products_count
            
        except Exception as e:
            print(f"âŒ é‡‡é›†å…³é”®è¯å¤±è´¥: {keyword} - {e}")
            self.logger.error(f"é‡‡é›†å…³é”®è¯å¤±è´¥: {keyword} - {e}")
            return 0
    
    def detect_captcha(self) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰éªŒè¯ç """
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜
            title = self.page.title
            if "Security Check" in title:
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯ç å…ƒç´ 
            captcha_elements = self.page.eles('tag:img')
            return len(captcha_elements) >= 2
            
        except Exception as e:
            self.logger.warning(f"éªŒè¯ç æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def handle_captcha(self) -> bool:
        """å¤„ç†éªŒè¯ç """
        try:
            # æŸ¥æ‰¾éªŒè¯ç å›¾ç‰‡
            images = self.page.eles('tag:img')
            if len(images) < 2:
                return False
            
            # è·å–èƒŒæ™¯å›¾å’Œæ»‘å—å›¾
            bg_img = images[0]
            slider_img = images[1]
            
            # ä¸‹è½½å›¾ç‰‡
            bg_url = bg_img.attr('src')
            slider_url = slider_img.attr('src')
            
            if not bg_url or not slider_url:
                return False
            
            # ä½¿ç”¨ddddocrè¯†åˆ«æ»‘å—ä½ç½®
            import requests
            bg_response = requests.get(bg_url)
            slider_response = requests.get(slider_url)
            
            if bg_response.status_code == 200 and slider_response.status_code == 200:
                target_x = self.det.slide_match(bg_response.content, slider_response.content)
                print(f"ğŸ¯ è¯†åˆ«åˆ°æ»‘å—ä½ç½®: {target_x}")
                
                # æŸ¥æ‰¾æ»‘å—å…ƒç´ å¹¶æ‹–æ‹½
                slider_element = self.page.ele('css:[class*="slider"]')
                if slider_element:
                    # è®¡ç®—å®é™…æ‹–æ‹½è·ç¦»
                    actual_distance = target_x * 0.6  # æ ¹æ®é¡µé¢ç¼©æ”¾è°ƒæ•´
                    
                    # æ‰§è¡Œæ‹–æ‹½
                    slider_element.drag((actual_distance, 0), duration=0.2)
                    time.sleep(2)
                    
                    return True
            
            return False
            
        except Exception as e:
            self.logger.error(f"éªŒè¯ç å¤„ç†å¤±è´¥: {e}")
            return False
    
    def extract_products(self, keyword: str) -> int:
        """æå–å•†å“æ•°æ®"""
        try:
            # æŸ¥æ‰¾é¡µé¢æ•°æ®è„šæœ¬
            script_elements = self.page.eles('tag:script')
            
            for script in script_elements:
                script_content = script.inner_html
                if 'window.__UNIVERSAL_DATA_FOR_REHYDRATION__' in script_content:
                    # è§£æé¡µé¢æ•°æ®
                    start_idx = script_content.find('{')
                    end_idx = script_content.rfind('}') + 1
                    
                    if start_idx != -1 and end_idx != -1:
                        json_str = script_content[start_idx:end_idx]
                        data = json.loads(json_str)
                        
                        # æå–å•†å“ä¿¡æ¯
                        products_count = self.parse_products_from_data(data, keyword)
                        return products_count
            
            return 0
            
        except Exception as e:
            self.logger.error(f"æå–å•†å“æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def parse_products_from_data(self, data: dict, keyword: str) -> int:
        """ä»é¡µé¢æ•°æ®ä¸­è§£æå•†å“"""
        try:
            products_count = 0
            
            # éå†æ•°æ®ç»“æ„æŸ¥æ‰¾å•†å“
            def find_products(obj, path=""):
                nonlocal products_count
                
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if key == "products" and isinstance(value, list):
                            # æ‰¾åˆ°å•†å“åˆ—è¡¨
                            for product in value:
                                if self.save_product(product, keyword):
                                    products_count += 1
                        else:
                            find_products(value, f"{path}.{key}")
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        find_products(item, f"{path}[{i}]")
            
            find_products(data)
            return products_count
            
        except Exception as e:
            self.logger.error(f"è§£æå•†å“æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def save_product(self, product: dict, keyword: str) -> bool:
        """ä¿å­˜å•†å“åˆ°æ•°æ®åº“"""
        try:
            # æå–å•†å“åŸºæœ¬ä¿¡æ¯
            product_id = product.get("product_id", "")
            title = product.get("title", "")
            
            if not product_id or not title:
                return False
            
            # æ„å»ºå•†å“æ•°æ®
            product_data = {
                "product_id": product_id,
                "title": title,
                "keyword": keyword,
                "scraped_at": datetime.now(),
                "slider_encountered": True,
                "slider_solved": True,
                "source": "tiktok_shop"
            }
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.collection.insert_one(product_data)
            print(f"ğŸ’¾ ä¿å­˜å•†å“: {title[:50]}...")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜å•†å“å¤±è´¥: {e}")
            return False
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.page:
                self.page.quit()
                print("âœ… æµè§ˆå™¨å·²å…³é—­")
            
            if self.mongo_client:
                self.mongo_client.close()
                print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")
    
    def run(self, keywords: str = "phone case", max_pages: int = 1):
        """
        è¿è¡Œçˆ¬è™«
        
        Args:
            keywords: å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰
            max_pages: æœ€å¤§é¡µæ•°
        """
        print("ğŸ‰ Crawlabç®€åŒ–çˆ¬è™«å¼€å§‹è¿è¡Œ")
        print("=" * 60)
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        if not self.setup_dependencies():
            return
        
        if not self.setup_database():
            return
        
        if not self.setup_browser():
            return
        
        if not self.setup_captcha_solver():
            return
        
        try:
            # å¤„ç†å…³é”®è¯åˆ—è¡¨
            keyword_list = [k.strip() for k in keywords.split(',')]
            total_products = 0
            
            for keyword in keyword_list:
                if keyword:
                    count = self.crawl_keyword(keyword, max_pages)
                    total_products += count
                    
                    # å…³é”®è¯é—´éš”
                    time.sleep(2)
            
            print("=" * 60)
            print(f"ğŸŠ çˆ¬è™«è¿è¡Œå®Œæˆï¼")
            print(f"âœ… å¤„ç†å…³é”®è¯: {len(keyword_list)} ä¸ª")
            print(f"âœ… é‡‡é›†å•†å“: {total_products} ä¸ª")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            self.logger.error(f"çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
        
        finally:
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    log_debug_info()
    
    # è·å–å‚æ•°
    keywords = os.getenv("keywords", "phone case")
    max_pages = int(os.getenv("max_pages", "1"))
    
    print(f"ğŸ“‹ è¿è¡Œå‚æ•°:")
    print(f"  å…³é”®è¯: {keywords}")
    print(f"  æœ€å¤§é¡µæ•°: {max_pages}")
    print()
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    crawler = SimpleCrawlabCrawler()
    crawler.run(keywords, max_pages)


if __name__ == "__main__":
    main()