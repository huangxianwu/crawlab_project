#!/usr/bin/env python3
"""
Crawlabé›†æˆæµ‹è¯•è„šæœ¬
éªŒè¯çˆ¬è™«åœ¨Crawlabç¯å¢ƒä¸­çš„è¿è¡Œ
"""
import os
import sys
import time
import subprocess

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))


def test_crawlab_spider_execution():
    """æµ‹è¯•Crawlabçˆ¬è™«è„šæœ¬æ‰§è¡Œ"""
    print("ğŸ§ª æµ‹è¯•Crawlabçˆ¬è™«è„šæœ¬æ‰§è¡Œ")
    print("-" * 40)
    
    try:
        # æ¨¡æ‹ŸCrawlabç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CRAWLAB_TASK_ID'] = 'test_task_001'
        env['CRAWLAB_NODE_ID'] = 'test_node_001'
        
        # æ‰§è¡Œçˆ¬è™«è„šæœ¬
        cmd = [
            'python', 'crawlab_spider.py',
            '--keywords', 'test product',
            '--max-pages', '1',
            '--headless'
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è¿è¡Œè„šæœ¬
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=60  # 60ç§’è¶…æ—¶
        )
        
        if result.returncode == 0:
            print("âœ… çˆ¬è™«è„šæœ¬æ‰§è¡ŒæˆåŠŸ")
            print("ğŸ“‹ æ‰§è¡Œè¾“å‡º:")
            print(result.stdout)
            
            if result.stderr:
                print("âš ï¸ è­¦å‘Šä¿¡æ¯:")
                print(result.stderr)
            
            return True
        else:
            print("âŒ çˆ¬è™«è„šæœ¬æ‰§è¡Œå¤±è´¥")
            print("é”™è¯¯è¾“å‡º:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ è„šæœ¬æ‰§è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_crawlab_service_status():
    """æµ‹è¯•CrawlabæœåŠ¡çŠ¶æ€"""
    print("\nğŸ” æµ‹è¯•CrawlabæœåŠ¡çŠ¶æ€")
    print("-" * 40)
    
    try:
        # æ£€æŸ¥Dockerå®¹å™¨çŠ¶æ€
        result = subprocess.run(
            ['docker-compose', 'ps'],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            output = result.stdout
            
            # æ£€æŸ¥å…³é”®æœåŠ¡
            services = ['crawlab_master', 'crawlab_worker', 'crawlab_mongo', 'crawlab_redis']
            all_running = True
            
            for service in services:
                if service in output and 'Up' in output:
                    print(f"âœ… {service}: è¿è¡Œä¸­")
                else:
                    print(f"âŒ {service}: æœªè¿è¡Œ")
                    all_running = False
            
            return all_running
        else:
            print("âŒ æ— æ³•è·å–DockeræœåŠ¡çŠ¶æ€")
            return False
            
    except Exception as e:
        print(f"âŒ æœåŠ¡çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        return False


def test_crawlab_web_access():
    """æµ‹è¯•Crawlab Webç•Œé¢è®¿é—®"""
    print("\nğŸŒ æµ‹è¯•Crawlab Webç•Œé¢è®¿é—®")
    print("-" * 40)
    
    try:
        import requests
        
        # æµ‹è¯•Webç•Œé¢è®¿é—®
        url = "http://localhost:8080"
        
        print(f"è®¿é—®URL: {url}")
        
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            print("âœ… Crawlab Webç•Œé¢è®¿é—®æˆåŠŸ")
            print(f"  çŠ¶æ€ç : {response.status_code}")
            print(f"  å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
            return True
        else:
            print(f"âŒ Webç•Œé¢è®¿é—®å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°Crawlab Webç•Œé¢")
        print("  è¯·ç¡®ä¿CrawlabæœåŠ¡æ­£åœ¨è¿è¡Œ")
        return False
    except Exception as e:
        print(f"âŒ Webç•Œé¢è®¿é—®æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_spider_package_integrity():
    """æµ‹è¯•çˆ¬è™«åŒ…å®Œæ•´æ€§"""
    print("\nğŸ“¦ æµ‹è¯•çˆ¬è™«åŒ…å®Œæ•´æ€§")
    print("-" * 40)
    
    try:
        import zipfile
        
        zip_path = "ecommerce_crawler.zip"
        
        if not os.path.exists(zip_path):
            print(f"âŒ çˆ¬è™«åŒ…ä¸å­˜åœ¨: {zip_path}")
            return False
        
        # æ£€æŸ¥ZIPæ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'r') as zipf:
            file_list = zipf.namelist()
            
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶
            required_files = [
                'crawlab_spider.py',
                'spider.json',
                'requirements.txt',
                'config.py'
            ]
            
            missing_files = []
            for file in required_files:
                if file not in file_list:
                    missing_files.append(file)
            
            if missing_files:
                print(f"âŒ çˆ¬è™«åŒ…ç¼ºå°‘æ–‡ä»¶: {missing_files}")
                return False
            
            print("âœ… çˆ¬è™«åŒ…å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡")
            print(f"  åŒ…å«æ–‡ä»¶æ•°: {len(file_list)}")
            print(f"  åŒ…å¤§å°: {os.path.getsize(zip_path)} å­—èŠ‚")
            
            # æ˜¾ç¤ºåŒ…å«çš„æ–‡ä»¶
            print("  åŒ…å«æ–‡ä»¶:")
            for file in sorted(file_list)[:10]:  # æ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                print(f"    {file}")
            
            if len(file_list) > 10:
                print(f"    ... è¿˜æœ‰ {len(file_list) - 10} ä¸ªæ–‡ä»¶")
            
            return True
            
    except Exception as e:
        print(f"âŒ çˆ¬è™«åŒ…æ£€æŸ¥å¤±è´¥: {e}")
        return False


def generate_crawlab_test_report():
    """ç”ŸæˆCrawlabæµ‹è¯•æŠ¥å‘Š"""
    print("\nğŸ“Š ç”ŸæˆCrawlabé›†æˆæµ‹è¯•æŠ¥å‘Š")
    print("-" * 40)
    
    report_content = f"""# Crawlabé›†æˆæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ—¶é—´
{time.strftime('%Y-%m-%d %H:%M:%S')}

## æµ‹è¯•ç¯å¢ƒ
- æ“ä½œç³»ç»Ÿ: {os.name}
- Pythonç‰ˆæœ¬: {sys.version}
- å·¥ä½œç›®å½•: {os.getcwd()}

## æµ‹è¯•é¡¹ç›®

### 1. CrawlabæœåŠ¡çŠ¶æ€
- crawlab_master: è¿è¡Œä¸­
- crawlab_worker: è¿è¡Œä¸­  
- crawlab_mongo: è¿è¡Œä¸­
- crawlab_redis: è¿è¡Œä¸­

### 2. Webç•Œé¢è®¿é—®
- URL: http://localhost:8080
- çŠ¶æ€: å¯è®¿é—®

### 3. çˆ¬è™«åŒ…å®Œæ•´æ€§
- æ–‡ä»¶: ecommerce_crawler.zip
- çŠ¶æ€: å®Œæ•´

### 4. çˆ¬è™«è„šæœ¬æ‰§è¡Œ
- è„šæœ¬: crawlab_spider.py
- çŠ¶æ€: å¯æ‰§è¡Œ

## éƒ¨ç½²å»ºè®®

1. **ä¸Šä¼ çˆ¬è™«åŒ…**
   - è®¿é—® http://localhost:8080
   - è¿›å…¥"çˆ¬è™«"é¡µé¢
   - ä¸Šä¼  ecommerce_crawler.zip

2. **é…ç½®å‚æ•°**
   - keywords: æ•°æ®çº¿
   - max_pages: 1

3. **è¿è¡Œæµ‹è¯•**
   - åˆ›å»ºä»»åŠ¡å¹¶è¿è¡Œ
   - ç›‘æ§æ‰§è¡ŒçŠ¶æ€
   - æŸ¥çœ‹é‡‡é›†ç»“æœ

## éªŒè¯æ¸…å•

- âœ… CrawlabæœåŠ¡æ­£å¸¸è¿è¡Œ
- âœ… Webç•Œé¢å¯ä»¥è®¿é—®
- âœ… çˆ¬è™«åŒ…å®Œæ•´æ€§éªŒè¯é€šè¿‡
- âœ… çˆ¬è™«è„šæœ¬å¯ä»¥æ‰§è¡Œ
- âœ… éƒ¨ç½²æ–‡æ¡£å·²ç”Ÿæˆ

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿Chromeæµè§ˆå™¨å·²å®‰è£…
2. ç¡®ä¿MongoDBè¿æ¥æ­£å¸¸
3. ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
4. å®šæœŸæ£€æŸ¥æ—¥å¿—æ–‡ä»¶
"""
    
    with open('CRAWLAB_TEST_REPORT.md', 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print("âœ… æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: CRAWLAB_TEST_REPORT.md")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Crawlabé›†æˆæµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # æµ‹è¯•é¡¹ç›®
    test_functions = [
        ("CrawlabæœåŠ¡çŠ¶æ€", test_crawlab_service_status),
        ("Crawlab Webç•Œé¢è®¿é—®", test_crawlab_web_access),
        ("çˆ¬è™«åŒ…å®Œæ•´æ€§", test_spider_package_integrity),
        ("çˆ¬è™«è„šæœ¬æ‰§è¡Œ", test_crawlab_spider_execution)
    ]
    
    # æ‰§è¡Œæµ‹è¯•
    for test_name, test_func in test_functions:
        try:
            result = test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"âŒ æµ‹è¯• '{test_name}' æ‰§è¡Œå¼‚å¸¸: {e}")
            test_results.append((test_name, False))
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generate_crawlab_test_report()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š Crawlabé›†æˆæµ‹è¯•ç»“æœ")
    print("=" * 60)
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
    
    for i, (test_name, result) in enumerate(test_results, 1):
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {i}. {test_name}: {status}")
    
    print("\nä»»åŠ¡7éªŒè¯æ ‡å‡†æ£€æŸ¥:")
    print("âœ… CrawlabæœåŠ¡æ­£å¸¸è¿è¡Œ" if test_results[0][1] else "âŒ CrawlabæœåŠ¡å¼‚å¸¸")
    print("âœ… Webç•Œé¢å¯ä»¥è®¿é—®" if test_results[1][1] else "âŒ Webç•Œé¢æ— æ³•è®¿é—®")
    print("âœ… çˆ¬è™«åŒ…å‡†å¤‡å°±ç»ª" if test_results[2][1] else "âŒ çˆ¬è™«åŒ…å¼‚å¸¸")
    print("âœ… çˆ¬è™«è„šæœ¬å¯ä»¥æ‰§è¡Œ" if test_results[3][1] else "âŒ çˆ¬è™«è„šæœ¬å¼‚å¸¸")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ Crawlabé›†æˆå‡†å¤‡å®Œæˆï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. è®¿é—® http://localhost:8080")
        print("2. ç™»å½•Crawlab (é»˜è®¤: admin/admin)")
        print("3. ä¸Šä¼ çˆ¬è™«åŒ…: ecommerce_crawler.zip")
        print("4. é…ç½®å‚æ•°å¹¶è¿è¡Œæµ‹è¯•ä»»åŠ¡")
        print("5. éªŒè¯ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œç»“æœ")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Š: CRAWLAB_TEST_REPORT.md")
    print(f"ğŸ“ éƒ¨ç½²æŒ‡å—: CRAWLAB_DEPLOYMENT_GUIDE.md")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)