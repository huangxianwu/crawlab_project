#!/usr/bin/env python3
"""
é€‚ç”¨äºCrawlabç¯å¢ƒçš„å®Œæ•´TikTok Shopçˆ¬è™«
åŒ…å«å®Œæ•´çš„æœç´¢ã€æ»‘å—å¤„ç†ã€å•†å“é‡‡é›†ã€ä¿å­˜åŠŸèƒ½
"""
import time
import json
import urllib.parse
import sys
import os
from datetime import datetime
from typing import List, Dict, Optional

# ç¡®ä¿é¡¹ç›®è·¯å¾„åœ¨Pythonè·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from DrissionPage import ChromiumPage, ChromiumOptions
    import ddddocr
    import requests
    import cv2
    import numpy as np
    import pymongo
    DEPENDENCIES_OK = True
except ImportError as e:
    print(f"âŒ ä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    DEPENDENCIES_OK = False

class CrawlabTikTokSpider:
    """
    Crawlabç¯å¢ƒä¸‹çš„TikTok Shopå®Œæ•´çˆ¬è™«
    é›†æˆæ‰€æœ‰åŠŸèƒ½ï¼šæœç´¢ã€æ»‘å—å¤„ç†ã€å•†å“é‡‡é›†ã€æ•°æ®ä¿å­˜
    """
    
    def __init__(self):
        self.page = None
        self.det = None
        self.mongo_client = None
        self.db = None
        self.collection = None
        self.is_running = True
        
        # é…ç½®ä¿¡æ¯
        self.mongo_uri = os.getenv("MONGO_URI", "mongodb://localhost:27017")
        self.database_name = os.getenv("DATABASE_NAME", "crawler_db")
        self.collection_name = os.getenv("COLLECTION_NAME", "products")
        
        print("ğŸš€ Crawlab TikTok Shopçˆ¬è™«åˆå§‹åŒ–")
        print(f"ğŸ“Š æ•°æ®åº“é…ç½®: {self.mongo_uri}")
        
    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        try:
            print("ğŸŒ æ­£åœ¨å¯åŠ¨Chromeæµè§ˆå™¨...")
            co = ChromiumOptions()
            
            # åŸºç¡€é…ç½®
            co.set_argument('--no-sandbox')
            co.set_argument('--disable-dev-shm-usage')
            co.set_argument('--disable-gpu')
            co.set_argument('--disable-web-security')
            co.set_argument('--allow-running-insecure-content')
            
            # åˆ›å»ºé¡µé¢å®ä¾‹
            self.page = ChromiumPage(co)
            self.page.set.user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0 Safari/537.36")
            self.page.set.load_mode.eager()
            
            print("âœ… æµè§ˆå™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æµè§ˆå™¨å¤±è´¥: {e}")
            return False
    
    def init_ocr(self):
        """åˆå§‹åŒ–OCR"""
        try:
            print("ğŸ” æ­£åœ¨åˆå§‹åŒ–éªŒè¯ç è¯†åˆ«...")
            if DEPENDENCIES_OK:
                self.det = ddddocr.DdddOcr(det=False, ocr=False)
                print("âœ… ddddocræ»‘å—æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                print("âŒ ddddocrä¾èµ–ä¸å¯ç”¨")
                return False
                
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–éªŒè¯ç è¯†åˆ«å¤±è´¥: {e}")
            return False
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            print("ğŸ”— è¿æ¥æ•°æ®åº“...")
            self.mongo_client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.mongo_client[self.database_name]
            self.collection = self.db[self.collection_name]
            
            # æµ‹è¯•è¿æ¥
            self.mongo_client.admin.command('ping')
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            total_count = self.collection.count_documents({})
            print(f"ğŸ“Š æ•°æ®åº“ä¸­ç°æœ‰å•†å“: {total_count} æ¡")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def navigate_to_url(self, url: str):
        """å¯¼èˆªåˆ°æŒ‡å®šURL"""
        try:
            print(f"ğŸ”„ è®¿é—®é¡µé¢: {url}")
            self.page.get(url)
            time.sleep(5)
            
            current_url = self.page.url
            current_title = self.page.title
            
            print(f"âœ… å½“å‰URL: {current_url}")
            print(f"âœ… é¡µé¢æ ‡é¢˜: {current_title}")
            
            return current_url, current_title
            
        except Exception as e:
            print(f"âŒ é¡µé¢å¯¼èˆªå¤±è´¥: {e}")
            return None, None
    
    def handle_captcha(self) -> bool:
        """å¤„ç†éªŒè¯ç """
        try:
            html_text = self.page.html
            has_security_check = "Security Check" in self.page.title
            
            if not has_security_check:
                return False
            
            print("ğŸ” æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæ­£åœ¨å¤„ç†...")
            
            # æŸ¥æ‰¾éªŒè¯ç å›¾ç‰‡
            imgs = self.page.eles("tag=img", timeout=20)
            print(f"é¡µé¢æ€»å…±æ‰¾åˆ° {len(imgs)} å¼ å›¾ç‰‡")
            
            # ç­›é€‰æ˜¾ç¤ºçš„å›¾ç‰‡
            visible_imgs = []
            for i, img in enumerate(imgs):
                try:
                    if img.states.is_displayed:
                        src = img.attr("src") or ''
                        size = img.rect.size
                        if size[0] > 50 and size[1] > 50:
                            visible_imgs.append(img)
                except:
                    continue
            
            print(f"ç­›é€‰å‡º {len(visible_imgs)} å¼ å¯è§çš„éªŒè¯ç å›¾ç‰‡")
            
            if len(visible_imgs) < 2:
                print("âš ï¸ éªŒè¯ç å›¾ç‰‡ä¸è¶³")
                return True
            
            # è·å–éªŒè¯ç å›¾ç‰‡URL
            background_img_url = visible_imgs[0].attr("src")
            target_img_url = visible_imgs[1].attr("src")
            
            print(f"èƒŒæ™¯å›¾URL: {background_img_url[:50]}...")
            print(f"æ»‘å—å›¾URL: {target_img_url[:50]}...")
            
            # ä¸‹è½½éªŒè¯ç å›¾ç‰‡
            background_response = requests.get(background_img_url, timeout=10)
            target_response = requests.get(target_img_url, timeout=10)
            
            if background_response.status_code == 200 and target_response.status_code == 200:
                # ä½¿ç”¨ddddocrçš„æ»‘å—åŒ¹é…åŠŸèƒ½
                background_bytes = background_response.content
                target_bytes = target_response.content
                
                try:
                    res = self.det.slide_match(target_bytes, background_bytes)
                    if res and "target" in res:
                        target_x = res["target"][0]
                        print(f"ğŸ¯ è¯†åˆ«åˆ°æ»‘å—ä½ç½®: {target_x}")
                        
                        # è®¡ç®—æ»‘å—ä½ç½®çš„åç§»é‡
                        x_offset = visible_imgs[1].rect.location[0] - visible_imgs[0].rect.location[0]
                        
                        # è·å–å›¾ç‰‡å°ºå¯¸è¿›è¡Œç¼©æ”¾
                        img_array = np.frombuffer(background_bytes, dtype=np.uint8)
                        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                        if img is not None:
                            height, width = img.shape[:2]
                            actual_x = target_x * (340 / width) - x_offset
                            print(f"ğŸ“ è®¡ç®—çš„å®é™…æ»‘åŠ¨è·ç¦»: {actual_x}")
                        else:
                            actual_x = target_x - x_offset
                        
                        # æ‰§è¡Œæ»‘åŠ¨æ“ä½œ
                        slider_element = self.page.ele("xpath://*[@id='secsdk-captcha-drag-wrapper']/div[2]", timeout=5)
                        if slider_element:
                            print(f"âœ… æ‰¾åˆ°æ»‘å—å…ƒç´ ï¼Œå¼€å§‹æ‹–æ‹½")
                            slider_element.drag(actual_x, 10, 0.2)
                            time.sleep(3)
                            
                            # æ£€æŸ¥éªŒè¯ç æ˜¯å¦é€šè¿‡
                            new_html = self.page.html
                            if "captcha-verify-image" not in new_html:
                                print("âœ… éªŒè¯ç å¤„ç†æˆåŠŸ")
                                return False
                            else:
                                print("âš ï¸ éªŒè¯ç æœªé€šè¿‡")
                        else:
                            print("âš ï¸ æœªæ‰¾åˆ°æ»‘å—å…ƒç´ ")
                    else:
                        print("âš ï¸ æ»‘å—ä½ç½®è¯†åˆ«å¤±è´¥")
                        
                except Exception as e:
                    print(f"âš ï¸ æ»‘å—è¯†åˆ«å¼‚å¸¸: {e}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ»‘å—å¤„ç†å¼‚å¸¸: {e}")
            return True
    
    def get_components_map(self) -> List[Dict]:
        """è·å–é¡µé¢ç»„ä»¶æ˜ å°„"""
        try:
            ele = self.page.ele("@id=__MODERN_ROUTER_DATA__", timeout=10)
            if not ele:
                print("âš ï¸ æœªæ‰¾åˆ°é¡µé¢æ•°æ®å…ƒç´ ")
                return []
            
            loader_data = json.loads(ele.inner_html)
            loader_keys = list(loader_data.get("loaderData", {}).keys())
            print(f"ğŸ” é¡µé¢ç»“æ„é”®: {loader_keys}")
            
            for key in loader_keys:
                if key and isinstance(loader_data["loaderData"][key], dict):
                    page_data = loader_data["loaderData"][key]
                    if "page_config" in page_data and "components_map" in page_data["page_config"]:
                        components_map = page_data["page_config"]["components_map"]
                        print(f"âœ… æ‰¾åˆ°é¡µé¢ç»„ä»¶æ˜ å°„: {len(components_map)} ä¸ªç»„ä»¶")
                        return components_map
            
            print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„é¡µé¢ç»“æ„")
            return []
            
        except Exception as e:
            print(f"âš ï¸ è§£æé¡µé¢ç»„ä»¶æ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_product_data(self, product: Dict, keyword: str) -> Optional[Dict]:
        """è§£æå•†å“æ•°æ®"""
        try:
            product_id = product.get("product_id", "")
            title = product.get("title", "")
            
            # ä»·æ ¼ä¿¡æ¯
            price_info = product.get("product_price_info", {})
            current_price_str = price_info.get("sale_price_format", "0")
            origin_price_str = price_info.get("origin_price_format", current_price_str)
            
            try:
                current_price = float(current_price_str.replace('$', '').replace(',', ''))
                origin_price = float(origin_price_str.replace('$', '').replace(',', ''))
            except:
                current_price = 0.0
                origin_price = 0.0
            
            # å…¶ä»–ä¿¡æ¯
            product_image = ""
            images = product.get("images", [])
            if images and len(images) > 0:
                product_image = images[0].get("url_list", [""])[0]
            
            sold_count = product.get("sold_count", 0)
            seller = product.get("seller", {})
            shop_name = seller.get("name", "")
            product_rating = product.get("product_rating", 0.0)
            review_count = product.get("review_count", 0)
            
            # åˆ›å»ºå•†å“æ•°æ®
            product_data = {
                'product_id': product_id,
                'title': title,
                'search_keyword': keyword,
                'current_price': current_price,
                'origin_price': origin_price,
                'shipping_fee': 0.0,
                'product_image': product_image,
                'product_url': f"https://www.tiktok.com/shop/product/{product_id}",
                'categories': "TikTok Shop",
                'desc_detail': "",
                'sold_count': sold_count,
                'product_rating': product_rating,
                'review_count': review_count,
                'review_count_str': str(review_count),
                'latest_review_fmt': "",
                'earliest_review_fmt': "",
                'shop_name': shop_name,
                'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'scraped_at': datetime.now().isoformat(),
                'slider_encountered': True,
                'slider_solved': True
            }
            
            return product_data
            
        except Exception as e:
            print(f"âŒ è§£æå•†å“æ•°æ®å¤±è´¥: {e}")
            return None
    
    def save_product_to_db(self, product_data: Dict):
        """ä¿å­˜å•†å“åˆ°æ•°æ®åº“"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = self.collection.find_one({"product_id": product_data['product_id']})
            if existing:
                return False
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            result = self.collection.insert_one(product_data)
            if result.inserted_id:
                print(f"ğŸ’¾ ä¿å­˜å•†å“: {product_data['title'][:30]}... - ${product_data['current_price']}")
                
                # è¾“å‡ºCrawlabæ ¼å¼çš„ç»“æœ
                crawlab_result = {
                    'product_id': product_data['product_id'],
                    'title': product_data['title'],
                    'price': product_data['current_price'],
                    'shop_name': product_data['shop_name'],
                    'scraped_at': product_data['scraped_at']
                }
                print(json.dumps(crawlab_result, ensure_ascii=False))
                
                return True
            else:
                return False
                
        except Exception as e:
            print(f"âŒ ä¿å­˜å•†å“åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def scrape_keyword_products(self, keyword: str, page_count: int = 2) -> List[Dict]:
        """å®Œæ•´çš„å•†å“é‡‡é›†æµç¨‹"""
        products = []
        
        try:
            # æ„å»ºæœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.tiktok.com/shop/s/{encoded_keyword}"
            
            print(f"ğŸŒ è®¿é—®TikTokæœç´¢é¡µé¢: {search_url}")
            
            # è®¿é—®æœç´¢é¡µé¢
            current_url, current_title = self.navigate_to_url(search_url)
            if not current_url:
                return products
            
            # å¤„ç†éªŒè¯ç 
            print("ğŸ§© æ£€æµ‹å’Œå¤„ç†æ»‘å—éªŒè¯...")
            if self.handle_captcha():
                print("âŒ éªŒè¯ç æ— æ³•è·³è¿‡ï¼Œåœæ­¢é‡‡é›†")
                return products
            
            print("âœ… æ»‘å—éªŒè¯å¤„ç†å®Œæˆï¼Œå¼€å§‹è§£æé¡µé¢æ•°æ®")
            
            # ç­‰å¾…é¡µé¢è·³è½¬
            time.sleep(5)
            
            # è·å–é¡µé¢ç»„ä»¶æ•°æ®
            print("ğŸ“Š æ­£åœ¨è§£æé¡µé¢æ•°æ®...")
            components_map = self.get_components_map()
            
            if not components_map:
                print("âš ï¸ æœªèƒ½è·å–é¡µé¢ç»„ä»¶æ•°æ®")
                return products
            
            # æå–ç¬¬ä¸€é¡µå•†å“åˆ—è¡¨
            for component in components_map:
                if component.get("component_name") == "feed_list_search_word":
                    component_products = component.get("component_data", {}).get("products", [])
                    print(f"ğŸ“¦ æ‰¾åˆ° {len(component_products)} ä¸ªå•†å“")
                    
                    for i, product in enumerate(component_products):
                        if not self.is_running:
                            break
                        
                        product_id = product.get("product_id")
                        if product_id:
                            print(f"ğŸ“¦ æ­£åœ¨å¤„ç†å•†å“ {i+1}/{len(component_products)}: {product_id}")
                            product_data = self.parse_product_data(product, keyword)
                            if product_data:
                                products.append(product_data)
                                self.save_product_to_db(product_data)
                    break
            
            print(f"ğŸ“¦ ç¬¬1é¡µè·å– {len(products)} ä¸ªå•†å“")
            
            # å¤„ç†æ›´å¤šé¡µé¢ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            if self.is_running and page_count > 1:
                print(f"ğŸ“„ å¼€å§‹è·å–æ›´å¤šé¡µé¢æ•°æ®ï¼Œé¢å¤–é¡µæ•°: {page_count - 1}")
                
                for page_num in range(page_count - 1):
                    try:
                        # æŸ¥æ‰¾"View more"æŒ‰é’®
                        view_more_btn = self.page.ele("text=View more", timeout=5)
                        if view_more_btn:
                            view_more_btn.click()
                            time.sleep(3)
                            print(f"ğŸ“„ å·²ç‚¹å‡»ç¬¬ {page_num + 2} é¡µ")
                        else:
                            print("âš ï¸ æœªæ‰¾åˆ°'View more'æŒ‰é’®ï¼Œåœæ­¢ç¿»é¡µ")
                            break
                    except:
                        break
            
            print(f"ğŸ‰ æ€»å…±é‡‡é›†åˆ° {len(products)} ä¸ªå•†å“")
            return products
            
        except Exception as e:
            print(f"âŒ é‡‡é›†å¤±è´¥: {e}")
            return products
    
    def run(self, keyword: str = "phone case", page_count: int = 2):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸ‰ Crawlab TikTok Shopå®Œæ•´çˆ¬è™«å¯åŠ¨")
        print("=" * 60)
        print(f"æœç´¢å…³é”®è¯: {keyword}")
        print(f"é‡‡é›†é¡µæ•°: {page_count}")
        print("=" * 60)
        
        try:
            # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
            if not self.init_browser():
                return False
            
            if not self.init_ocr():
                return False
            
            if not self.init_database():
                return False
            
            # è·å–åˆå§‹å•†å“æ•°é‡
            initial_count = self.collection.count_documents({})
            
            # å¼€å§‹é‡‡é›†
            start_time = time.time()
            products = self.scrape_keyword_products(keyword, page_count)
            end_time = time.time()
            
            # è·å–æœ€ç»ˆå•†å“æ•°é‡
            final_count = self.collection.count_documents({})
            new_products = final_count - initial_count
            
            # æ˜¾ç¤ºç»“æœ
            duration = end_time - start_time
            print(f"\nğŸ“Š é‡‡é›†ç»“æœæ±‡æ€»:")
            print(f"  âœ… é‡‡é›†å…³é”®è¯: {keyword}")
            print(f"  âœ… é‡‡é›†é¡µæ•°: {page_count}")
            print(f"  âœ… é‡‡é›†å•†å“æ•°: {len(products)}")
            print(f"  âœ… æ–°å¢å•†å“æ•°: {new_products}")
            print(f"  âœ… æ•°æ®åº“æ€»å•†å“: {final_count}")
            print(f"  âœ… é‡‡é›†è€—æ—¶: {duration:.2f} ç§’")
            
            print(f"\nğŸŠ Crawlabçˆ¬è™«ä»»åŠ¡å®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {e}")
            return False
        
        finally:
            self.close()
    
    def close(self):
        """å…³é—­èµ„æº"""
        try:
            if self.page:
                self.page.quit()
            if self.mongo_client:
                self.mongo_client.close()
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
        except:
            pass

def main():
    """ä¸»å‡½æ•°"""
    # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    keyword = os.getenv('CRAWLAB_KEYWORDS', 'phone case')
    page_count = int(os.getenv('CRAWLAB_MAX_PAGES', '2'))
    
    # åˆ›å»ºå¹¶è¿è¡Œçˆ¬è™«
    spider = CrawlabTikTokSpider()
    success = spider.run(keyword, page_count)
    
    if success:
        print("ğŸ‰ çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        sys.exit(0)
    else:
        print("âŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    if not DEPENDENCIES_OK:
        print("âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„åŒ…")
        sys.exit(1)
    
    main()